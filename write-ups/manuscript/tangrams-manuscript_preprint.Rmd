---
title: |
  | Interaction structure constrains 
  | the emergence of conventions in group communication

author:
  - name: Veronica Boyce
    email: vboyce@stanford.edu 
    affiliation:
      - Stanford Psych
  - name: Robert D. Hawkins
    affiliation: 
      - UW-Madison
  - name: Noah D. Goodman
    affiliation: 
      - Stanford Psych
      - Stanford CS
  - name: Michael C. Frank 
    affiliation: 
      - Stanford Psych
address:
  - code: Stanford Psych
    address: Psychology Department, Stanford University, Stanford, CA 94305
  - code: Stanford CS
    address: Computer Science Department, Stanford University, Stanford, CA 94305
  - code: UW-Madison
    address: Psychology Department, University of Wisconsin -- Madison, Madison, WI, 53715
bibliography: ["papers.bib"] # Replace with one or more of your own bibtex files. Better BibTeX for Zotero is your friend
csl: apa7.csl # Use any CSL style. See https://www.zotero.org/styles for a good list. Ignored if citation_package: natbib
link-citations: TRUE
header-includes:
  - \usepackage{tikz}
  - \usetikzlibrary{angles,positioning,arrows.meta, quotes, shapes, shapes.geometric}
  - \usepackage{graphicx}
  - \usepackage{emoji}
  - \usepackage{caption, subcaption}
output:
  bookdown::pdf_document2:
    toc: FALSE
    keep_tex: TRUE
    template: generic_article_template.tex
    #md_extensions: "-autolink_bare_uris"
    latex_engine: lualatex
    number_sections: FALSE
    citation_package: default # Can also be "natbib"
lang: en # Main document language in BCP47 format
geometry: "margin=1.25in"
linestretch: 1 # for double spacing
endfloat: FALSE # Set to TRUE to turn on latex endfloat package to place figures and tables at end of document
# endfloatoption: # See endfloat documentation for more possibilities
#   - tablesfirst # Default
#   - nomarkers # Default
numberlines: FALSE
authblk: TRUE # FALSE = author affiliations in footnotes; TRUE = author affiliations in a block below author names
footnotehyper: FALSE # TRUE will give you enhanced table footnote capabilities. Set to FALSE to be able to use French blocks. Needed due to what appears to be a latex bug.
urlcolor: blue
linkcolor: blue
citecolor: blue
graphics: TRUE # Needed to be able to include images
tables: TRUE # Needed to be able to include tables

---
Corresponding author: Veronica Boyce, vboyce@stanford.edu

<!---------------------- Abstract --------------------->


::: {.abstract data-latex="" lang=en}

Real-world communication frequently requires language producers to address more than one comprehender at once, yet most psycholinguistic research focuses on one-on-one communication.
As the audience size grows, interlocuters face new challenges that do not arise in dyads. 
They must consider multiple perspectives and weigh multiple sources of feedback to build shared understanding.
Here, we ask which properties of the group's *interaction structure* facilitate successful communication.
We used a repeated reference game paradigm in which directors instructed between one and five matchers to choose specific targets out of a set of abstract figures. 
Across 313 games ($N=1,319$ participants), we manipulated several key constraints on the group's interaction, including the amount of feedback that matchers could give to directors and the availability of peer interaction between matchers. 
Across groups of different sizes and interaction constraints, describers produced increasingly efficient utterances and matchers made increasingly accurate selections. 
Critically, however, we found that smaller groups and groups with less-constrained interaction structures ("thick channels") showed stronger convergence to group-specific conventions than large groups with constrained interaction structures ("thin channels"), which struggled with convention formation. 
Overall, these results shed new light on the core structural factors that enable communication to thrive in larger groups.

:::

Significance Statement: Human linguistic communication is remarkably versatile, from one-on-one conversations to large group chats on messaging platforms. Yet existing empirical work has overwhelmingly focused on one-on-one contexts, overlooking the factors that affect real-world communication in larger groups. We address this gap using a large-scale reference game task, examining how groups of varying sizes and interaction structures coordinate on linguistic conventions for novel objects. We found effective and robust communication across multiple settings, but larger (6-person) groups were more sensitive to differences in group structure and communication channels. Our results suggest that "thicker" communication channels - where group members provide more feedback to one another - can result in better communication in larger groups.
<!------------ Main text -------------------->

```{r set-up, include=FALSE}
require(Hmisc)#not directly called, but needed for dots on plots with mean_ci_boot ! 
#this has to go first b/c it defines summarize and that screws things up
library(tidyverse)
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(jsonlite)
library(here)
library(rlang)
library(lme4)
library(brms)
library(rstan)
library(viridis)
library(cowplot)
library(tidybayes)
library(kableExtra)
 

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

knitr::opts_chunk$set(fig.width=8, fig.height=8, fig.crop = F, out.width = "100%", dpi=300,
                      fig.pos = "tb", fig.path='figs/', fig.env="figure",
                      echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)

color_scheme_1 <- c("2"="#FFBDD4", "5"="#A12EFF", "3"="#FF7DF0","6"="#6940FF","4"="#D24AFF")
color_scheme_2 <- c( "6 full feedback"="#425df5", "6 same describer"="#00A2FF","6 thin"="#D47E04")

color_scheme_3 <- c("2 thin"="#FFDA09","6 thin"="#D47E04", 
                    "2 thick"="#77F3DB","6 thick"="#00BDA8")
		
ParseJSONColumn <- function(x) {
  str_c("[ ", str_c(x, collapse = ",", sep=" "), " ]")  |> 
    fromJSON(flatten = T)
}

##Data import constants
data_location="data/study1"
image_location="write-ups/images"
msum_loc="code/paper_mods/summary"
mform_loc="code/paper_mods/formulae"

source(here("code/prep_ms.R"))


stats <- function(model, row, decimal=2){
  model <- model |> 
    mutate(Estimate=round(Estimate, digits=decimal),
           Lower=round(lower, digits=decimal),
           Upper=round(upper, digits=decimal),
           `Credible Interval`=str_c("[",Lower,", ", Upper,"]")) |> 
    select(Term, Estimate, `Credible Interval`)
  str_c("$\\beta=", model[row,2],",\\:95\\%\\:\\mathrm{CrI}=",model[row,3], "$")
}

stats_text  <- function(model, row, decimal=2){
    model <- model |> 
    mutate(Estimate=round(Estimate, digits=decimal),
           Lower=round(lower, digits=decimal),
           Upper=round(upper, digits=decimal),
           `Credible Interval`=str_c("[",Lower,", ", Upper,"]")) |> 
    select(Term, Estimate, `Credible Interval`)
  str_c("", model[row,2],", ($95\\%\\:\\mathrm{CrI}=",model[row,3], "$)")
}

form <- function(model){
 dep <- as.character(model$formula[2])
 ind <- as.character(model$formula[3])
 
 str_c(dep," ~ ",ind) |> str_replace_all(" ","") |> 
  str_replace_all("\\*"," $\\\\times$ ") |> 
  str_replace_all("\\+", "&nbsp;+ ") |> 
   str_replace_all("~", "$\\\\sim$ ")
}
```

```{r count, include=F}

games <- combined_results |> select(gameId) |> unique() |> nrow() 

players <- combined_results |> select(gameId, numPlayers) |> unique() |> 
summarize(players=sum(numPlayers))

words <- combined_chat |> ungroup() |> select(total_num_words) |> summarize(words=sum(total_num_words)) 
```

# Introduction

Much of human social life revolves around communication in groups. 
At school, teachers address large classrooms of children [@cazden1988classroom]; at home, we chat with groups of friends and family members over dinner [@tannen2005conversational]; and at work, we attend meetings with colleagues and managers [@caplow1957organizational; @zack1993interactivity].
Such settings present considerable challenges that do not arise in the purely two-party (dyadic) settings typically studied in psychology [@branigan2006; @ginzburg2005; @traum2004]. 
For example, producers need to account for the fact that different comprehenders in the group may have different mental states or levels of background understanding [@horton2005; @horton2002; @yoon2018; @yoon2014; @weber2003;@fox-tree2013], while comprehenders must account for the fact that utterances are not necessarily tailored to them [@fay2000; @carletta1998; @metzing2003; @yoon2019; @rogers2013; @cohngordon; @tolins2016].
What enables producers and comprehenders to nevertheless overcome these challenges and navigate multi-party settings with relative ease? 

One promising set of hypotheses centers on the group's *interaction structure*, the set of constraints placed on the group's shared communication channel. 
Many different aspects of interaction structure have been implicated in the effectiveness of dyadic communication, including the availability and quality of concurrent feedback [@krauss1966; @KraussBricker67_Delay; @kraut1982listener], the bandwidth of the communication modality [@KraussEtAl77;@dewhirst1971influence], and the group's access to a shared workspace [@clark2004speaking; @garrod2007foundations].
Yet larger groups introduce qualitatively different dimensions of interaction structure, leading to a large but often inconsistent body of findings even for these well-understood factors [@swaab2012communication; @hiltz1986experiments].
While communication is generally expected to deteriorate as groups get larger [@macmillan_communication_2004; @seaman1997communication], the structural "thickness" of the feedback channel may slow such deterioration [@ahern1994effect; @parisi2005evaluating]. 

In this paper, we develop an experimental paradigm for evaluating the relative contribution of these factors: a *multi-party repeated reference game.*
The ability to distinguish one particular entity from other possible entities, known as *reference*, is one of the most primitive and ubiquitous functions of communication.
Reference games [@Wittgenstein1953; @lewis1969convention] have been widely used to study dyadic communication under controlled conditions in the lab. 
They provide a clear metric of communicative effectiveness: how many words are required before a matcher successfully chooses a target image from a context of distractors? 
*Repeated* reference games, where the same target images appear multiple times in succession, were introduced to examine how interlocutors establish shared reference in the absence of conventional labels [@krauss1964; @clark1986].
At the beginning of the game, long and costly descriptions are typically required to succeed. 
A key finding, however, is that dyads become increasingly efficient over the course of interaction. 
Fewer words are required to achieve the same accuracy, but referring expressions also become more impenetrable to outsiders [@schober1989; @wilkes1992coordinating]. 
The evolution of referring expressions over repetitions shows the characteristic dynamics of conventions: *stability*, or convergence on labels within a group, and *arbitrariness*, or divergence to different across groups, suggesting that dyads leverage their shared communication history to coordinate on expectations about how to label the target images [@hawkins2023partners]. 


In principle, repeated reference games provide a strong operationalization of communicative effectiveness for the problem of multi-party communication: describers must simultaneously achieve shared reference with multiple matchers. 
However, empirically studying multi-party communication raises a number of difficulties in practice. 
A much larger pool of participants must be recruited to achieve sufficient power at the relevant unit of analysis -- the group -- spanning a very high-dimensional space of possible parameter settings [@almaatouq2022].
We address this problem by drawing on recent technical advances that have made it newly possible to achieve such samples using interactive web-based platforms [@almaatouq2020empirica;@haber2019;@hawkins2023partners]. 
Repeated reference games in web-based platforms have previously replicated earlier results from face-to-face studies, although people produce fewer words in text modalities than oral modalities [@hawkins2020]. The text-based chat modalities arguably more closely resemble the interfaces used by modern teams who increasingly communicate through group text threads or popular platforms like Slack or Discord. 

We leverage our platform to explore effects of group size and interaction channel thickness in a series of three experiments. 
While we find that small groups reliably converge on group-specific "shorthand" regardless of the interaction structure, larger groups require thicker channels -- richer conversational feedback among members -- to achieve the same degree of coherence. 
Thus, increasing group size alone does not impede communication; rather, larger groups may require stronger social and linguistic cues to establish common ground among all members.
More broadly, our work suggests that studying communication in larger groups is necessary to unveil critical aspects of interaction structure that have not been evident in typical dyadic settings.


```{r diagram,  fig.env = "figure*", fig.pos = "t!", fig.width=10, fig.height=8, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap="(A) Participants played a repeated reference game in groups of size 2 to 6. On each trial, a describer described the target image to the group of matchers. Each image appeared once per block for six blocks. (B) Experiments varied along 3 dimensions: Group size, group coherence, and matcher contributions. (C) Experiment 1 (pink) varied group size from 2 to 6 players while holding group coherence and matcher contributions constant. Experiment 2 (blue) held group size constant at 6 and manipulated the other dimensions. Experiment 3 (green) tested 4 corners of the space, crossing group size (2 vs. 6 players) with the thickness of interaction structure (high vs. low coherence and matcher contributions)."}
knitr::include_graphics("expt-diagram4.pdf")
```

# Results

We recruited `r players` participants through Prolific, an online crowd-sourcing platform.
Participants were organized into `r games` groups of size two to six for a communication game (Figure \@ref(fig:behavioral)A).
On each trial, everyone in the group was shown a gallery of 12 tangram images [@clark1986; @hawkins2020; @ji2022abstract].
One player was designated the *describer* and the others were designated the *matchers*. 
The describer was asked to use a chat box interface to describe a privately indicated *target* image. 
After all matchers guessed which of the 12 images was the target, they received task feedback and proceeded to the next trial.
The game consisted of 72 trials structured into 6 repetition blocks, where each image appeared as the target exactly once per block.

We manipulated the interaction structure of this game across 11 distinct conditions in 3 distinct pre-registered experiments (Figure \@ref(fig:behavioral)B). 
We systematically sampled points along four dimensions parameterizing different aspects of the interaction space. 
We manipulated *group size* (ranging from two to six), *role stability* (whether or not participants took turns in the describer role), richness of *task feedback* (whether or not matchers were able to see each other's responses), and richness of the *matcher contributions* (whether matchers were able to freely respond through a chatbox or could only use emojis; Figure \@ref(fig:behavioral)C).
Other factors, such as the set of stimuli and background knowledge about one's partners, were held constant across games. 

### Overview of experiments 

Experiment 1 began by investigating how performance scaled with group size.
Based on prior qualitative work, we predicted that larger groups face a more challenging coordination problem. 
We continuously varied the number of players from 2 to 6 while keeping other factors constant. 
For these conditions, the describer role rotated after each block, so that all players had at least one turn as describer.
Matchers had access to an unrestricted chat box, but only received binary task feedback about whether their individual selection was correct without revealing others' selections or the intended target. 

Experiment 2 focused on the most challenging 6-player groups and explored the role of interaction structure. Each condition in Experiment 2 varied one aspect of the experiment relative to the Experiment 1 6-player baseline. We tried two variants that we expected to increase group coherence and improve performance, and a third variant we expected to interfere with the ability to establish mutual understanding and thus impede performance. 
In the first variant, we maintained the same describer throughout rather than a rotating describer, such that the same individual has the opportunity to aggregate feedback across trials and track which matchers are struggling with which targets. 
In the second variant, we gave the group of matchers full feedback about what every other member of the group had selected, and we showed the intended target. 
In the third variant, we changed how matchers could make contributions to the group. In contrast to prior experiments, where matchers could contribute freely to the chat; here, we limited matchers to sending four discrete emojis (green check, thinking face, red x, and laughing-crying face) that could convey simple valence and level of comprehension, but not any referential content.

Experiment 3 crossed the extremes of group size from experiment 1 (2 vs. 6 people) with the extremes of group interactions from Experiment 2 (*thick* vs. *thin* interaction structure). 
In the *thick* condition, we maintained a consistent describer, gave all matchers full task feedback, and allowed them to freely use a chat box. 
In the *thin* condition, we forced the describer to rotate on each block, restricted feedback to their own binary correctness, and restricted matcher contributions to the four emojis.
Note that the 2-player thick game most closely resembles the design of classic repeated reference games [@clark1986].

```{r behavioral, fig.env = "figure*", fig.pos = "t!", fig.width=10, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Behavioral results across all three experiments. (A-C). Matcher accuracy at selecting the target image. (D-F). Number of words produced by the describer each trial. For all, small dots are per game, per block means, and smooth lines are predictions from model fixed effects with 95\\% credible intervals. Y-axes are truncated, and a few outliers points are not visible."}
# accuracy

acc_pred_1 <- read_rds(here("code/paper_mods/prediction/acc_pred_1.rds"))
acc_pred_2 <- read_rds(here("code/paper_mods/prediction/acc_pred_2.rds"))
acc_pred_3 <- read_rds(here("code/paper_mods/prediction/acc_pred_3.rds"))
# 1
one_acc_dat <- combined_results |> 
  filter(condition=="rotate") |> 
  group_by(playerId,repNum, gameId, numPlayers) |> 
  filter(response!="false") |> 
  mutate(correct.num=ifelse(correct,1,0)) |> 
  mutate(numPlayers=as.character(numPlayers))

one_acc_plot <- ggplot(one_acc_dat, aes(x=repNum+1, y=correct.num, color=numPlayers))+
     scale_x_continuous(breaks=seq(1,6))+
     coord_cartesian(ylim=c(.6,1.05))+
  geom_point(data=one_acc_dat |> group_by(repNum, gameId, numPlayers) |> summarize(correct.num=mean(correct.num)),position = position_dodge(width=.4), alpha=.3)+
      #geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
  geom_lineribbon(data=acc_pred_1, aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(numPlayers,after_scale=alpha(fill,.2))))+
    labs(x="Block", y="Fraction correctly selected", color="", title="", subtitle="Experiment 1")+
    guides(color = guide_legend(nrow=2, byrow=T, override.aes = list(linetype = 0, alpha=1, fill=NA, size=1.2) ) )+
    #annotate("text", x=1,y=1.05,label="A", size=6, fontface="bold")+
    theme(legend.position="none",
          axis.text=element_text(size=12),
          legend.text=element_text(size=13),
          axis.title=element_text(size=14),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"))+
    scale_color_manual(values=color_scheme_1, aesthetics=c("color", "fill"))

two_acc_dat <- combined_results |>
  filter(condition %in% c("no_rotate","full_feedback", "emoji")) |>
  filter(numPlayers==6) |> 
  group_by(playerId,repNum, gameId, numPlayers) |> 
  filter(response!="false") |> 
  mutate(correct.num=ifelse(correct,1,0)) |> 
  mutate(condition=case_when(
    condition=="no_rotate" ~ "6 same describer",
    condition=="full_feedback" ~ "6 full feedback",
    condition=="emoji" ~ "6 thin"),
    condition=factor(condition, levels=c("6 full feedback", "6 same describer", "6 thin"))
  )

two_acc_plot <- ggplot(two_acc_dat, aes(x=repNum+1, y=correct.num, color=condition))+
    scale_x_continuous(breaks=seq(1,6))+
    coord_cartesian(ylim=c(.6,1.05))+
  geom_point(data=two_acc_dat |> group_by(repNum, gameId, condition) |> summarize(correct.num=mean(correct.num)),position = position_dodge(width=.4), alpha=.3)+
    #geom_smooth(method = "glm", method.args = list(family = "binomial")) +
   geom_lineribbon(data=acc_pred_2, aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))))+
    labs(x="", y="", color="", title="Accuracy", subtitle="Experiment 2")+
    #annotate("text", x=1,y=1.05,label="B", size=6, fontface="bold")+     
    #annotate("text", x=3.5,y=1.05,label="Accuracy", size=6)+
    theme(legend.position="none",
          axis.text=element_text(size=12),
          legend.text=element_text(size=14),
          axis.title=element_text(size=14),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),
          axis.title.y=element_blank())+ 
    guides(color = guide_legend(nrow=2, byrow=F, override.aes = list(linetype = 0, alpha=1, fill=NA, size=1.2) ) )+
    scale_color_manual(values=color_scheme_2, aesthetics=c("color", "fill"))

#3
three_acc_dat <- combined_results |> 
  filter(condition %in% c("2_thin", "2_thick", "6_thin", "6_thick")) |> 
  group_by(playerId,repNum, gameId, condition) |> 
  filter(response!="false") |> 
  mutate(correct.num=ifelse(correct,1,0)) |>
  mutate(condition=str_replace(condition, "_", " "),
         condition=factor(condition, levels=c("2 thick", "6 thick", "2 thin", "6 thin"))) 

three_acc_plot <-  ggplot(three_acc_dat, aes(x=repNum+1, y=correct.num, color=condition))+
    scale_x_continuous(breaks=seq(1,6))+
    coord_cartesian(ylim=c(.6,1.05))+
    geom_point(data=three_acc_dat |> group_by(repNum, gameId, condition) |> summarize(correct.num=mean(correct.num)),
               position = position_dodge(width=.4), alpha=.3)+
    #geom_smooth(method = "glm", method.args = list(family = "binomial")) + 
    geom_lineribbon(data=acc_pred_3, 
                     aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))))+
    #annotate("text", x=1,y=1.05,label="C", size=6, fontface="bold")+
    labs(x="", y="", color="", title="", subtitle="Experiment 3")+
    theme(legend.position="none",
          axis.text=element_text(size=12),
          legend.text=element_text(size=14),
                   plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),
          axis.title=element_text(size=14),
          axis.title.y=element_blank())+
    guides(color = guide_legend(nrow=2, byrow=T, override.aes = list(linetype = 0, alpha=1, fill=NA, size=1.1) ) )+
    scale_color_manual(values=color_scheme_3, aesthetics = c("color", "fill"))
 
acc <- plot_grid(one_acc_plot, two_acc_plot, three_acc_plot, nrow=1, rel_widths = c(1.05, 1, 1), labels=c("A", "B", "C"), label_size=20, label_y=.9)
 

red_pred_1 <- read_rds(here("code/paper_mods/prediction/red_pred_1.rds"))
red_pred_2 <- read_rds(here("code/paper_mods/prediction/red_pred_2.rds"))
red_pred_3 <- read_rds(here("code/paper_mods/prediction/red_pred_3.rds"))
#1
one_red_dat <- combined_chat |> 
  filter(condition=="rotate") |>  
  filter(role=="speaker") |> 
  mutate(groupxtangram=str_c(gameId,tangram)) |> 
  group_by(repNum, gameId,tangram, groupxtangram, numPlayers) |> 
  summarize(words=sum(total_num_words)) |>
  mutate(numPlayers=as.character(numPlayers))

one_red_plot <-  ggplot(one_red_dat, aes(x=repNum+1, y=words, color=numPlayers))+
      geom_point(data=one_red_dat |> group_by(repNum, gameId, numPlayers) |> summarize(words=mean(words)),position = position_dodge(width=.4), alpha=.3)+
    #geom_smooth(method=glm, formula=y~poly(x,2), alpha=.3)+
   geom_lineribbon(data=red_pred_1, aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(numPlayers,after_scale=alpha(fill,.2))))+
    coord_cartesian(ylim=c(0,45))+
    scale_x_continuous(breaks=seq(1,6))+
    labs( y="Number of words", x="Block", color="", title="", subtitle="Experiment 1")+
    guides(color = guide_legend(nrow=2, byrow=F, override.aes = list(linetype = 0, alpha=1, fill=NA, size=5)), fill=FALSE  )+
    #annotate("text", x=1,y=45,label="D", size=6, fontface="bold")+
    theme(legend.position="bottom",
          axis.text=element_text(size=12),
          legend.text=element_text(size=13),
          axis.title=element_text(size=14),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),          axis.title.x=element_blank())+
    scale_color_manual(values=color_scheme_1, aesthetics=c("color","fill"))


#2

two_red_dat <- combined_chat|> 
  filter(condition %in% c("no_rotate","full_feedback", "emoji")) |>
  filter(numPlayers==6) |> 
  filter(role=="speaker") |> 
  mutate(groupxtangram=str_c(gameId,tangram)) |> 
  group_by(repNum, gameId,tangram, groupxtangram,condition) |> 
  summarize(words=sum(total_num_words)) |> 
  mutate(condition=case_when(
    condition=="no_rotate" ~ "6 same describer",
    condition=="full_feedback" ~ "6 full feedback",
    condition=="emoji" ~ "6 thin"),
    condition=factor(condition, levels=c("6 full feedback", "6 same describer", "6 thin"))
  )

two_red_plot <- ggplot(two_red_dat,aes(x=repNum+1, y=words, color=condition))+
    #geom_point(size=1, alpha=.05, position = position_dodge(width=.4))+
    guides(color = guide_legend(nrow=2, byrow=F, override.aes = list(linetype = 0, alpha=1, fill=NA, size=5)), fill=FALSE  )+
 geom_point(data=two_red_dat |> group_by(repNum, gameId, condition) |> summarize(words=mean(words)),position = position_dodge(width=.4), alpha=.5)+
    #geom_smooth(method=glm, formula=y~poly(x,2), alpha=.3)+
   geom_lineribbon(data=red_pred_2, aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))))+
    coord_cartesian(ylim=c(0,45))+
    scale_x_continuous(breaks=seq(1,6))+
    labs(x="", y="", color="", title="Words from describer", subtitle="Experiment 2")+
    #annotate("text", x=1,y=45,label="E", size=6, fontface="bold")+
    #annotate("text", x=3.5,y=45,label="Words from describer", size=6)+
    theme(legend.position="bottom",
          axis.text=element_text(size=12),
          legend.text=element_text(size=13),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),          axis.title=element_blank())+
    scale_color_manual(values=color_scheme_2, aesthetics=c("color", "fill"))

#3
three_red_dat <- combined_chat |> 
  filter(condition %in% c("2_thin", "2_thick", "6_thin", "6_thick")) |> 
  filter(role=="speaker") |> 
  mutate(groupxtangram=str_c(gameId,tangram)) |> 
  group_by(repNum, gameId,tangram, groupxtangram, condition) |> 
  summarize(words=sum(total_num_words)) |> 
  mutate(condition=str_replace(condition, "_", " "),
         condition=factor(condition, levels=c("2 thick", "6 thick", "2 thin", "6 thin")))


three_red_plot <-  ggplot(three_red_dat, aes(x=repNum+1, y=words, color=condition))+
     geom_point(data=three_red_dat |> group_by(repNum, gameId, condition) |> summarize(words=mean(words)),
                position = position_dodge(width=.4), alpha=.3)+
    #geom_smooth(method=glm, formula=y~poly(x,2), alpha=.3)+
   geom_lineribbon(data=red_pred_3, aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))))+
    guides(color = guide_legend(nrow=2, byrow=F, override.aes = list(linetype = 0, alpha=1, fill=NA, size=5)), fill=FALSE )+
    coord_cartesian(ylim=c(0,45))+
    scale_x_continuous(breaks=seq(1,6))+
    labs(x="", y="", color="", title="", subtitle="Experiment 3")+
    #annotate("text", x=1,y=45,label="F", size=6, fontface="bold")+
    theme(legend.position="bottom",
          axis.text=element_text(size=12),
          legend.text=element_text(size=13),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),          axis.title=element_blank())+
    scale_color_manual(values=color_scheme_3, aesthetics=c("color", "fill"))

red <- plot_grid(one_red_plot,two_red_plot, three_red_plot, nrow=1, rel_widths = c(1.05,1,1), labels=c("D", "E", "F"), label_size=20, label_y=.9)
 plot_grid(acc, red, nrow=2, rel_heights = c(.95,1.1))


```

```{r}
acc_1 <- read_rds(here(msum_loc,"acc_1.rds"))

acc_2a <- read_rds(here(msum_loc,"acc_2a.rds"))

acc_2b <- read_rds(here(msum_loc,"acc_2b.rds"))

acc_2c <- read_rds(here(msum_loc,"acc_2c.rds"))

acc_3 <- read_rds(here(msum_loc,"acc_3.rds"))

acc_mega <- read_rds(here(msum_loc,"acc_meta.rds"))
```

### Smaller and higher-coherence groups are more accurate

Our first set of hypotheses focused on group performance: how accurately and efficiently groups were able to perform the referential task. 
We characterize group performance along two complementary metrics: (1) matcher accuracy and (2) describer efficiency. 
Matcher accuracy is given by the percent of matchers on each trial who successfully selected the target referent. 
Describer efficiency is given by the number of words produced by the describer to achieve that degree of matcher accuracy in the group.
The degree to which describers are able to communicate more efficiently without negatively impacting matcher accuracy is indicative of convergence on a more effective shared communication protocol within the group.

We begin by examining matcher accuracy, the extent to which the intended target was reliably transmitted to all matchers.
We constructed a series of 5 logistic mixed-effects regression models predicting accuracy as a function of condition and repetition block (separate models were run for experiment 1, each condition in experiment 2, and experiment 3). For this and other effects, there was substantial variation at the tangram and game levels, with some tangrams being markedly easier than others and some groups performing differently than others. This wide variation made it difficult to precisely estimate population-level main effects, leading to wide credible intervals. See SI Figure 11 for a visualization of the relative magnitudes of population effects and game and tangram level variations.  

Across all conditions, we observed strong positive effects of repetition block, indicating improved performance over time (Figure \@ref(fig:behavioral)A-C, SI Tables 4-8).
In Experiment 3, larger games began with lower initial accuracy (`r stats(acc_3,7)`) and improved more slowly (`r stats(acc_3,4)`) than smaller games, although group size differences were not reliable in Experiment 1 (SI Table 4), and these experiment 3 differences were not robust in a sensitivity analysis (SI Figure 4C and SI Table 58). 
Among large groups in Experiment 2, accuracy was higher in the thicker conditions than in the condition with thin interaction structure (SI Tables 5-7), although effects of game thickness were not reliable in Experiment 3 (SI Table 8). 

Because each experiment only explored a slice of the full parameter space, we also considered an exploratory analysis that pooled data across experiments, aiming to mitigate the loss in power from running entirely separate regression models. 
Specifically, we aggregated data from all experiments into a post-hoc mega-analytic model predicting accuracy as a function of repetition block, game thickness (thin v. not-thin) and game size. 
Overall, we found evidence that accuracy increased over time (`r stats(acc_mega,2)`) but the rate of increase was reduced for thin games (`r stats(acc_mega, 4)`) and larger games (`r stats(acc_mega,3)`) compared to smaller or thicker games. 
That is, smaller groups and groups with higher coherence tended to be more accurate, though the magnitude and reliability of these effects varied across individual experiments.

```{r}
red_1 <- read_rds(here(msum_loc, "red_1.rds"))

red_2a <- read_rds(here(msum_loc,"red_2a.rds"))

red_2b <- read_rds(here(msum_loc,"red_2b.rds"))

red_2c <- read_rds(here(msum_loc,"red_2c.rds"))

red_3 <- read_rds(here(msum_loc, "red_3.rds"))

red_mega <- read_rds(here(msum_loc,"mega_red.rds"))
red_3_extra <- read_rds(here(msum_loc, "red_3_extra")) %>% select(diff6, thin2, thin6, thick6)

diff6 <- str_c("$\\beta=", round(red_3_extra[2,1],2),",\\:95\\%\\:\\mathrm{CrI}=[",round(red_3_extra[1,1],2),", ",round(red_3_extra[3,1],1), "]$")

thin2 <- str_c("$\\beta=", round(red_3_extra[2,2],2),", (95\\%\\:\\mathrm{CrI}=[",round(red_3_extra[1,2],2),", ",round(red_3_extra[3,2],1), "]$)")

thin6 <- str_c("$\\beta=", round(red_3_extra[2,3],2),", (95\\%\\:\\mathrm{CrI}=[",round(red_3_extra[1,3],2),", ",round(red_3_extra[3,3],1), "]$)")

thick6 <- str_c("$\\beta=", round(red_3_extra[2,4],2),", (95\\%\\:\\mathrm{CrI}=[",round(red_3_extra[1,4],2),", ",round(red_3_extra[3,4],1), "]$)")
```

### Smaller and higher-coherence groups are more efficient

After establishing that groups were able to communicate accurately, we turned to the challenges faced by describers when deciding how much information to provide.
Specifically, we predicted that larger and more heterogeneous groups may initially require more information, but that thicker interaction structure may similarly allow describers to communicate more effectively over time.
We tested these predictions using linear mixed-effects models predicting the number of words a describer produced on each trial as a function of condition and block. 
These models counted all words the describer produced, including after matcher contributions (similar effects were found in models predicting the length of describer's utterances before any matcher contributions, see SI Tables 21-24).

First, as predicted, describers in larger groups produced longer descriptions at the outset than describers in smaller groups (Figure \@ref(fig:behavioral)D-F). 
This effect held for the continuous manipulation of group size for Experiment 1 (`r stats(red_1,4)`) as well as the 2-person versus 6-person manipulation in Experiment 3 (`r stats(red_3,8)`).
Smaller groups also continued to use shorter descriptions than larger groups over the course of the game. 
In Experiment 1, the rate at which efficiency increased was similar across different size groups (`r stats(red_1,3)`). 
In Experiment 3, larger groups reduced faster than smaller ones (`r stats(red_3, 5)`), but the faster reduction did not fully make up for the longer initial starting point, and was not robust to a sensitivity analysis (SI Figure 4F and SI Table 63). 

 
While thin 6-person games showed a flatter reduction trajectory than thicker 6-person games in Experiment 2 (SI Tables 10-12), there was no reliable effect of game thickness on reduction in Experiment 3 (SI Table 13). 

The reduction patterns of description lengths is paralleled by how long matchers took to make selections; across conditions, matchers selected faster in later conditions (SI Figure 9), and the correlation between speed and description length was consistent across experiments (SI Figure 10). 

Aggregating across experiments with a mega-analytic model, however, suggested that larger games were associated with steeper reduction (`r stats(red_mega,3)`) from a more verbose starting point (`r stats(red_mega,6)`) than smaller games, and thin games had shallower reduction curves (`r stats(red_mega,4)`) than thicker games. 
Overall, then, smaller games used shorter descriptions than larger games across various time points in the experiment, and thinner games reduced less than thicker games. 


```{r}
list_1 <- read_rds(here(msum_loc, "list_1.rds"))
list_spec_1 <- read_rds(here(mform_loc, "list_1.rds"))

anylist_1 <- read_rds(here(msum_loc, "anylist_1.rds"))
anylist_spec_1 <- read_rds(here(mform_loc, "anylist_1.rds"))

per_list_1 <- read_rds(here(msum_loc,"per_list_1.rds"))
per_anylist_1 <- read_rds(here(msum_loc,"per_anylist_1.rds"))
```

```{=latex}


\begin{table}
	\centering

	\caption{Examples from 6-player groups in Experiment 3 of successful descriptions for the same image across repetitions. Describers are indicated with an asterisk. More example descriptions are in SI Tables 1 and 2. \\\label{listener-examples}}
	\begin{subtable}{0.5\linewidth}
		\centering
			\begin{tabular}{lp{2.5in}}
			\hline
			
			\multicolumn{2}{c}{\textbf{6-person thick game}}\\
			\multicolumn{2}{l}{\textit{Rep 1: 4/5 correct}}\\
			A*  &   sitting down no legs showing   \\                
			C   & no arms?\\                       
			B  &    Bunny ears?  \\                                  
			A*  &   legs showing to one side no arms  \\             
			C  &    kinda like kneeling?  \\                         
			A*  &   no feet      \\                                  
			A*   &  yes kneeling  \\
			\multicolumn{2}{l}{\textit{Rep 2: 5/5 correct}}\\
			A*  &   sitting down with no feet showing. legs to one side \\
			A*  &  no arms   \\                                     
			E &      Swaddled up like a bb \\                         
			C  &    cute bb    \\                                    
			D  &    I think that's the one that looks like a ghost to me, like he's wearing a sheet. (?)\\
			\multicolumn{2}{l}{\textit{Rep 3: 5/5 correct}}\\
			A*  &    sitting down no arms showing legs to one side  \\
			E  &   the bb   \\                                      
			B  &  Swadled baby?  \\                                
			F &    Baby?   \\   
			A* &    the bb     \\ 
			\multicolumn{2}{l}{\textit{Rep 4: 5/5 correct}}\\
			A*    & sitting down with no arms legs to one side \\
			\multicolumn{2}{l}{\textit{Rep 5: 5/5 correct}}\\
			A* & sitting no arms and legs to one side. I will call them Kevin \\
			D & <3 Kevin\\
			C & bb kevin \\
			\multicolumn{2}{l}{\textit{Rep 6: 5/5 correct}}\\
      A* & kevin the baby\\
      E & yess kevin\\
      			\hline
		\end{tabular}
	\end{subtable}%
	\hspace*{2em}
	\begin{subtable}{0.5\linewidth}
		\centering
		\begin{tabular}{lp{2.5in}}
		\multicolumn{2}{c}{\includegraphics[width=1in]{images/tangram_H.png}}\\
		\hline

			\multicolumn{2}{c}{\textbf{6-person thin game}}\\
			\multicolumn{2}{l}{\textit{Rep 1: 5/5 correct}}\\
			L* & man with no arms or legs \\
			O & 	 \emoji{thinking-face}\\
			P & 	 \emoji{thinking-face}\\
			L* & slight protuberance on the right \\
			\multicolumn{2}{l}{\textit{Rep 2: 5/5 correct}}\\
			N* & 	Can't see any arms. Imagine wrapped in a blanket completely. \\
			N* & Armless and legless \\
			N* & Burrito with a head \\
			M & 		 \emoji{face-with-tears-of-joy}\\
			O & 	 \emoji{face-with-tears-of-joy}\\
			P & 			 \emoji{thinking-face}\\
			\multicolumn{2}{l}{\textit{Rep 3: 5/5 correct}}\\	
			O* & burrito \\
			\multicolumn{2}{l}{\textit{Rep 4: 5/5 correct}}\\
			Q* & burrito \\
			\multicolumn{2}{l}{\textit{Rep 5: 5/5 correct}}\\
			M* & burrito\\
						\multicolumn{2}{l}{\textit{Rep 6: 5/5 correct}}\\
			P* & And our last one! Anyone else hungry after this? I quite fancy a burrito!\\
			L & \emoji{face-with-tears-of-joy}\\
			Q & \emoji{check-mark-button}\\
			N & \emoji{face-with-tears-of-joy}\emoji{face-with-tears-of-joy}\emoji{face-with-tears-of-joy}\\
			Q & \emoji{face-with-tears-of-joy}\emoji{check-mark-button}\\
			M & \emoji{check-mark-button}\\
			Q & \emoji{face-with-tears-of-joy}\\
			N & \emoji{check-mark-button}\emoji{check-mark-button}\emoji{check-mark-button}\\
						\hline
		\end{tabular}
	\end{subtable}
\end{table}


```


```{r, include=F}

#supplement
listener_sample <- pre_chat_3 |> filter(target=="/experiment/tangram_H.png") |> filter(gameId=="E8n7PT7sEpCcbb7AD") |> 
  select(playerId, repNum, text, countCorrect, role) |> 
  mutate(playerId=factor(playerId, labels=c("A", "B", "C", "D", "E", "F"))) |> 
  mutate(id=str_c(playerId, ifelse(role=="speaker", "*", ""))) |> select(id, repNum, text, countCorrect)
  
# text thick "bb"
listener_sample <- pre_chat_3 |> filter(target=="/experiment/tangram_H.png") |> 
  filter(gameId=="BTbGhXZvjdSFubTBg") |>   select(playerId, repNum, text, countCorrect, role)

listener_sample <- pre_chat_3 |> filter(target=="/experiment/tangram_H.png") |> 
  filter(gameId=="2nZ79PA8WMtYNqLrS") |>   select(playerId, repNum, text, countCorrect, role)
listener_sample <- pre_chat_3 |> filter(target=="/experiment/tangram_H.png") |>  filter(gameId=="RBnjoHwwmyq7ojwD7") |> 
  select(playerId, repNum, text, countCorrect, role) |> 
  mutate(playerId=factor(playerId, labels=c("G", "H", "I", "J", "K", "L"))) |> 
  mutate(id=str_c(playerId, ifelse(role=="speaker", "*", ""))) |> select(id, repNum, text, countCorrect)

listener_sample <- pre_chat_3 |> filter(target=="/experiment/tangram_H.png") |> 
  filter(gameId=="GWCD2NGoiA2n5Wh2i") |> 
  select(playerId, repNum, text, countCorrect, role) |> 
  mutate(playerId=factor(playerId, labels=c("M", "N", "O", "P", "Q"))) |> 
  mutate(id=str_c(playerId, ifelse(role=="speaker", "*", ""))) |> select(id, repNum, text, countCorrect)

listener_sample <- pre_chat_3 |> filter(target=="/experiment/tangram_H.png") |>  filter(gameId=="kR9MDbh9ktZFmJkcK") |> 
  select(playerId, repNum, text, countCorrect, role) |> 
  mutate(playerId=factor(playerId, labels=c("V", "U", "W", "X", "Y"))) |> 
  mutate(id=str_c(playerId, ifelse(role=="speaker", "*", ""))) |> select(id, repNum, text, countCorrect)



```

### Larger groups make greater use of matcher contributions

As a final measure of group performance, we examined the back-and-forth interactions between the describer and the group of matchers.
Matchers use their chat contributions to actively provide feedback, ask questions, offer alternative descriptions, and seek clarification about the describer's referring expressions. 
Example transcripts from successful games, one in the 6-thick condition and one in the 6-thin condition, are shown in Table \ref{listener-examples}. Additional examples are in the SI Tables 1 and 2. 
Overall, we found that larger groups displayed a higher proportion of trials where at least one matcher produced utterances (SI Figure 6A, `r stats(anylist_1, 4)`), which declined across repetition blocks (`r stats(anylist_1, 2)`). On an individual level, a matcher in a larger group was more likely to make contributions than a matcher in a smaller group, although each contribution tended to be shorter (SI Figure 7, SI Tables 18, 20).
The length of matcher interjections also decreased over time, especially for large groups (SI Figure 6D, `r stats(list_1,3)`) consistent with the need for early matcher involvement in establishing referential conventions. 
Emoji use in Experiment 3 followed similar trends (SI Figure 8).
Overall, describers in larger groups receive more total input from matchers, suggesting larger groups may require greater participation by matchers to reliably establish common ground.


### Descriptions converge faster in groups with thicker channels

```{r sbert-diagram, fig.env = "figure*", fig.pos = "t!", fig.width=10, fig.height=8, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Example utterances describing the shown tangram figure produced by two 3-player games in Experiment 1. To measure convergence within a game (blue), we measured the cosine similarity between SBERT embeddings of descriptions and the embedding of the round 6 utterance (taken to be the convention). Higher cosine similarity indicates more similar meaning. To measure divergence between games (green), we measured the similarity between embeddings of utterances from the same round across games."}
knitr::include_graphics("sbert.pdf")
```

In the previous sections, we examined three metrics of communicative performance in groups of different sizes and interaction structures. 
We confirmed that groups in all conditions replicated the classic patterns of increasing accuracy and decreasing description length.
We also found some initial evidence that larger groups may struggle to improve performance in the absence of thick communication channels.
Here, we aim to better understand the mechanisms that allow describers to use shorter descriptions without sacrificing accuracy. 
In particular, we explore the hypothesis that interaction structure and group size affect performance through a *convention formation* process [@clark1986]. 
Under a recent model of convention formation [@hawkins2023partners], groups are able to leverage their shared history to coordinate on stable expectations about how to refer to particular images.
This model makes specific predictions about how interaction structure affects the ability to coordinate, in terms of the available feedback.

First, due to heterogeneity in the group -- 6 individuals who may have diverging conceptualizations --- a rational describer should provide a strictly more detailed initial description to hedge against multiple possible misunderstandings, as we previously observed. 
Second, all groups should display the characteristic dynamics of conventions: *stability*, or convergence within group, and *arbitrariness*, or divergence to multiple equilibria across groups. 
Third, convergence should be faster when a single individual is consistently in the describer role and when matchers are able to freely respond in natural language, as describers are able to aggregate feedback about the effectiveness of their own utterances from block to block and also immediately correct specific misunderstandings within a given trial.

To assess the dynamics of describer descriptions, we examine the *semantic similarity* of descriptions within and across games.
We quantified description similarity by concatenating describer messages together within a trial and embedding this description into a high-dimensional vector space using SBERT. 
SBERT is a BERT-based sentence embedder designed to map semantically similar sentences to embeddings that are nearby in embedding space.
Semantically meaningful comparisons between sentences are made by taking pairwise cosine similarities between the embeddings [@reimers2019]. 

To measure stability, or convergence within groups, we compared utterances from blocks one through five to the final (block six) description for the same image from the same game. 
To measure arbitrariness, or divergence across groups depending on group-specific history, we compared utterances produced by different describers for the same image in the corresponding blocks. 
Figure \@ref(fig:sbert-diagram) illustrates these two measures with example utterances and their within-game and between-game cosine similarities. 

```{r sbert, fig.env = "figure*", fig.pos = "t!", fig.width=10, fig.height=8.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Language similarity results measured with pairwise cosine similarity between embeddings of two utterances. (A-C). Convergence of descriptions within games as measured by similarity between an utterance from block 1-5 to the block 6 utterance in the same game for the same image. (D-F). Divergence of descriptions across games as measured by the similarity between two utterances produced for the same image by different groups in the same block. For all, small dots are per game, per block means, and smooth lines are predictions from model fixed effects with 95\\% credible intervals. Y-axes are truncated, and a few outliers points are not visible."}
#convergence
one_two_converge <- read_rds(here("code/models/one_two_converge.rds"))
three_converge <- read_rds(here("code/models/three_converge.rds"))

#1

one_conv_dat <- one_two_converge |> filter(condition %in% c("2", "3","4","5","6"))
conv_pred_1 <- read_rds(here("code/paper_mods/prediction/conv_pred_1.rds"))
conv_pred_2 <- read_rds(here("code/paper_mods/prediction/conv_pred_2.rds"))
conv_pred_3 <- read_rds(here("code/paper_mods/prediction/conv_pred_3.rds"))
one_conv_plot <- ggplot(one_conv_dat, aes(x=earlier+1,y=sim,color=condition))+
  geom_point(position = position_dodge(width=.4), alpha=.3,
             data=one_conv_dat |> group_by(earlier, gameId, condition) |> summarize(sim=mean(sim)))+
  geom_lineribbon(aes(x=earlier+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))),
                  data=conv_pred_1)+
  coord_cartesian(ylim=c(.2,1))+
  scale_x_continuous(breaks=seq(1,6))+
  labs(y="Cosine Similarity", x="Block", color="", title="", subtitle="Experiment 1")+
  guides(color = guide_legend(nrow=2, byrow=T, override.aes = list(linetype = 0, alpha=1, fill=NA, size=6) ) )+
  theme(legend.position="none",
        axis.text=element_text(size=12),
        legend.text=element_text(size=13),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),        axis.title=element_text(size=14))+
  scale_color_manual(values=color_scheme_1, aesthetics=c("color", "fill"))


#2

two_conv_dat <- one_two_converge |>  filter(condition %in% c("6emoji", "6highfeed", "6noro")) |> 
    mutate(condition=case_when(
    condition=="6noro" ~ "6 same describer",
    condition=="6highfeed" ~ "6 full feedback",
    condition=="6emoji" ~ "6 thin"))

two_conv_plot <- ggplot(two_conv_dat, aes(x=earlier+1,y=sim,color=condition))+
  geom_point(position = position_dodge(width=.4), alpha=.3,
             data=two_conv_dat |> group_by(earlier, gameId, condition) |> summarize(sim=mean(sim)))+
  geom_lineribbon(aes(x=earlier+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))),
                  data=conv_pred_2)+
  scale_x_continuous(breaks=seq(1,6))+
  coord_cartesian(ylim=c(.2,1))+
  labs(x="", y="", color="", title="Within game", subtitle="Experiment 2")+
  theme(legend.position="none",
        axis.text=element_text(size=12),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),        legend.text=element_text(size=14),
        axis.title=element_text(size=14),
        axis.title.y=element_blank())+    
  scale_color_manual(values=color_scheme_2, aesthetics=c("color", "fill"))



#3
three_conv_dat <- three_converge |> mutate(condition=str_c(str_sub(condition,1,1)," ", str_sub(condition,2,-1)))
three_conv_plot <- ggplot(three_conv_dat, aes(x=earlier+1,y=sim,color=condition))+
  geom_point(position = position_dodge(width=.4), alpha=.3,
             data=three_conv_dat |> group_by(earlier, gameId, condition) |> summarize(sim=mean(sim)))+
  geom_lineribbon(aes(x=earlier+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))),
                  data=conv_pred_3)+
  scale_x_continuous(breaks=seq(1,6))+
  coord_cartesian(ylim=c(.2,1))+
  guides(color = guide_legend(nrow=2, byrow=T, override.aes = list(linetype = 0, alpha=1, fill=NA, size=6) ) )+
  labs(x="", y="", color="", title="", subtitle="Experiment 3")+
  theme(legend.position="none",
        axis.text=element_text(size=12),
        legend.text=element_text(size=14),
        axis.title=element_text(size=14),
          plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),
        axis.title.y = element_blank())+
  scale_color_manual(values=color_scheme_3, aesthetics=c("color", "fill"))

conv <- plot_grid(one_conv_plot, two_conv_plot, three_conv_plot, nrow=1, rel_widths = c(1.05, 1,1), labels=c("A", "B", "C"), label_y=.9, label_size = 20)
one_two_diverge <- read_rds(here("code/models/one_two_diverge.rds"))
three_diverge <- read_rds(here("code/models/three_diverge.rds"))

# divergence
div_pred_1 <- read_rds(here("code/paper_mods/prediction/div_pred_1.rds"))
div_pred_2 <- read_rds(here("code/paper_mods/prediction/div_pred_2.rds"))
div_pred_3 <- read_rds(here("code/paper_mods/prediction/div_pred_3.rds"))

#1
one_div_dat <-  one_two_diverge |> filter(condition %in% c("2", "3","4","5","6"))
one_div_plot <- ggplot(one_div_dat,aes(x=repNum+1,y=sim,color=condition))+
  geom_point(position = position_dodge(width=.4), alpha=.3,
             data=one_div_dat |> group_by(repNum, tangram, condition) |>  summarize(sim=mean(sim)))+
  geom_lineribbon(aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))),
                  data=div_pred_1)+
  guides(color = guide_legend(nrow=2, byrow=F, override.aes = list(linetype = 0, alpha=1, fill=NA, size=5) ), fill=F )+
  scale_x_continuous(breaks=seq(1,6))+
        coord_cartesian(ylim=c(.1,.7))+
  labs(y="Cosine Similarity", x="Block", color="", subtitle="Experiment 1", title="")+
  theme(legend.position="bottom",
        axis.title.x=element_blank(),
        axis.text=element_text(size=12),
                  plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),
        legend.text=element_text(size=13),
        axis.title=element_text(size=14))+
  scale_color_manual(values=color_scheme_1, aesthetics=c("color", "fill"))

#2
two_div_dat <-  one_two_diverge |>  filter(condition %in% c("6emoji", "6highfeed", "6noro")) |>
  mutate(condition=case_when(
    condition=="6noro" ~ "6 same describer",
    condition=="6highfeed" ~ "6 full feedback",
    condition=="6emoji" ~ "6 thin"),
     condition=factor(condition, levels=c("6 full feedback", "6 same describer", "6 thin")))
 
two_div_plot <- ggplot(two_div_dat, aes(x=repNum+1,y=sim,color=condition))+
  geom_point(data=two_div_dat |> group_by(repNum, tangram, condition) |>
               summarize(sim=mean(sim)),position = position_dodge(width=.4), alpha=.3)+
  geom_lineribbon(data=div_pred_2, aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))))+
  scale_x_continuous(breaks=seq(1,6))+
  coord_cartesian(ylim=c(.1,.7))+
  labs(x="", y="", color="", title="Between games", subtitle="Experiment 2")+
  guides(color = guide_legend(nrow=2, byrow=F, override.aes = list(linetype = 0, alpha=1, fill=NA, size=5) ) , fill=F)+
  theme(legend.position="bottom",
        axis.title.x=element_blank(),
        axis.text=element_text(size=12),
        legend.text=element_text(size=13),
                  plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),
        axis.title=element_text(size=14),
        axis.title.y=element_blank())+
   scale_color_manual(values=color_scheme_2, aesthetics=c("color", "fill"))

#3
three_div_dat <-  three_diverge |> 
  mutate(condition=str_c(str_sub(condition,1,1)," ", str_sub(condition,2,-1)),                    
         condition=factor(condition, levels=c("2 thick", "6 thick", "2 thin", "6 thin")))

three_div_plot <- ggplot(three_div_dat, aes(x=repNum+1,y=sim,color=condition))+
  geom_point(data=three_div_dat |> group_by(repNum, tangram, condition) |>
                   summarize(sim=mean(sim)),position = position_dodge(width=.4), alpha=.3)+
  geom_lineribbon(data=div_pred_3, aes(x=block+1,ymin=low, y=mean, ymax=high, fill=stage(condition,after_scale=alpha(fill,.2))))+
  scale_x_continuous(breaks=seq(1,6))+
  coord_cartesian(ylim=c(.1,.7))+
  labs(x="", y="", color="", subtitle="Experiment 3", title="")+
  guides(color = guide_legend(nrow=2, byrow=F, override.aes = list(linetype = 0, alpha=1, fill=NA, size=5) ), fill=F )+
  theme(legend.position="bottom",
        axis.title.x=element_blank(),
        axis.text=element_text(size=12),
        legend.text=element_text(size=13),
                  plot.subtitle=element_text(size=16, hjust=.5),
          plot.title=element_text(size=18, hjust=.5, face="bold"),
        axis.title=element_text(size=14),
        axis.title.y=element_blank())+
  scale_color_manual(values=color_scheme_3, aesthetics=c("color", "fill"))

div <- plot_grid(one_div_plot,two_div_plot,three_div_plot, nrow=1, rel_widths = c(1.05, 1, 1), labels=c("D", "E", "F"), label_y=.9, label_size=20)
plot_grid(conv, div, nrow=2,  rel_heights = c(.95,1.1))
```

```{r}
tolast_1 <- read_rds(here(msum_loc, "tolast_1.rds"))

tonext_1 <- read_rds(here(msum_loc, "tonext_1.rds"))

tolast_2a <- read_rds(here(msum_loc, "tolast_2a.rds"))

tonext_2a <- read_rds(here(msum_loc, "tonext_2a.rds"))

tolast_2b <- read_rds(here(msum_loc, "tolast_2b.rds"))

tonext_2b <- read_rds(here(msum_loc, "tonext_2b.rds"))

tolast_2c <- read_rds(here(msum_loc, "tolast_2c.rds"))

tonext_2c <- read_rds(here(msum_loc, "tonext_2c.rds"))

tolast_3 <- read_rds(here(msum_loc, "tolast_3.rds"))

tonext_3 <- read_rds(here(msum_loc, "tonext_3.rds"))

tolast_mega <- read_rds(here(msum_loc,"mega_tolast.rds"))
```

We modeled semantic convergence with a mixed effects linear regression model predicting the similarity between a block 1-5 utterance and the corresponding block 6 utterance as a function of the earlier block number and condition (Figure \@ref(fig:sbert)A-C; SI Tables 25-29). 
All conditions showed some convergence toward a conventional ``shorthand" for the picture, but the speed of convergence was affected both by group size and channel width.
First, we found that smaller groups reached stable descriptions faster than larger games. 
In Experiment 1, initial similarity was invariant across group size (`r stats(tolast_1, 2,3)`), but smaller groups converged faster (Figure \@ref(fig:sbert)A, `r stats(tolast_1, 4,3)`). 
In Experiment 3, 6-person thick games started off further from their eventual convention than 2-person thick games (`r stats(tolast_3, 8,3)`) but closed the gap over time (Figure \@ref(fig:sbert)C, `r stats(tolast_3, 7,3)`, this effect was not robust to sensitivity analysis, SI Figure 5C and SI Table 68). 
Second, thicker games tended to converge faster than thin games (Figure \@ref(fig:sbert)B-C). 
In Experiment 3, small thin games started off slightly further from their convention than small thick games, and this gap widened over time (`r stats(tolast_3, 5,3)`).
Finally, the combination of thin interaction structure and larger group hindered convergence more than either factor individually. 
Beyond the generally slower convergence in thin games, 6-person thin games showed substantially slower convergence even compared to 2-person thin games in Experiment 3 (`r stats(tolast_3, 6,3)`). 

Pooling across experiments in a mega-analysis confirms this pattern. 
Thin games converge less than thick games overall (`r stats(tolast_mega, 5,3)`), and *large* thin games are especially slow to converge (`r stats(tolast_mega, 4,3)`). 
Across games, convergence towards the last utterance was driven by cumulative increasing similarity between pairs of utterances in adjacent blocks (SI Figure 12D-F, SI Tables 40-44). 
In early rounds, descriptions could change substantially between rounds, but by later rounds, many descriptions had already reduced and solidified and varied little round to round. 
In summary, we found that stable descriptions emerged earlier if the group was smaller, or if the group had a thick interaction structure.

```{r}
div_1 <- read_rds(here(msum_loc, "div_1.rds"))

div_2a <- read_rds(here(msum_loc, "div_2a.rds"))

div_2b <- read_rds(here(msum_loc, "div_2b.rds"))

div_2c <- read_rds(here(msum_loc, "div_2c.rds"))

div_3 <- read_rds(here(msum_loc, "div_3.rds"))

div_mega <- read_rds(here(msum_loc,"mega_div.rds"))
```


### Games with thicker channels diverge from one another more quickly 

While groups may initially overlap in their descriptions, including details of shapes or body parts, we predicted that their descriptions would become increasingly dissimilar as groups increasingly adapt to their own idiosyncratic shared history. 
To test this effect, we constructed a mixed-effects linear regression model predicting the cross-game similarity between a pair of utterances for the same image. 
A decrease in the similarity between different groups descriptions occurred in every condition, indicating increasing arbitrariness and group-specificity of descriptions (Figure \@ref(fig:sbert)D-F, SI Tables 30-34).
However, different game sizes and interaction structures revealed very different strengths of divergence.

First, smaller games used more group-specific language.
In Experiment 1, smaller games diverged more quickly than larger games (`r stats(div_1,3,3)`). 
In Experiment 3, 2-person thick games started off more dissimiliar than 6-person thick games, although 6-person games diverged faster and eventually approached the dissimilarity levels of 2-person thick games (SI Table 34). 
Second, thicker interaction structure was associated with stronger group-specific divergence. 
In Experiment 3, 2-person thin games diverged more slowly than 2-person thick games (`r stats(div_3, 3,3)`). 
As with the convergence patterns, large games with thin interaction structures had the flattest trajectories, as thinness and largeness compounded.
In Experiment 3, 6-person thin games diverged even less than 2-player thin games (Figure \@ref(fig:sbert)F, `r stats(div_3, 4,3)`), and in Experiment 2, 6-person thin games barely diverged at all (Figure \@ref(fig:sbert)E, `r stats(div_2c,2,3)`). 
A mega-analytic model confirms this pattern: thin games differentiate less between groups (`r stats(div_mega,4,3)`) and large thin groups differentiate even less (`r stats(div_mega, 5,3)`).

As a complement to the embedding analysis, we also examined the frequency of a few classes of words in the descriptions. Literal geometric words (ex. square, triangle, etc) and words for body parts (leg, arm, etc) are common early in games, but decline over repetition in most conditions, to be replaced by more abstract descriptions that do not contain these classes of words (SI Figure 2). The 6-person thin condition, however, retains a higher level of literal geometric and body part words, along with high levels of positional words (above, left, below, etc) and posture words (kicking, standing, seated, etc), with a lower level of utterances that do not contain any of these classes of words.

# Discussion

From classrooms to boardrooms, human communication often takes place in multi-party settings. 
However, experimental research rarely focuses on such settings, largely due to practical obstacles. 
<!-- Dyadic reference games have been used to measure informational efficiency, characterized by describer-matcher pairs creating conventional (stable but somewhat arbitrary) labels which are not shared by other groups.  -->
In the current work, we asked how convention formation processes, typically studied in dyadic reference games, unfold in larger groups and under varying interaction structures. 
Across 3 online experiments and 11 experimental conditions, we varied multiple features of interaction structure including group size, modality of matcher contributions, and degree of group coherence. 
All conditions replicated classic dyadic phenomena: increasing accuracy and efficiency, semantic convergence within games, and differentiation of descriptions between groups. 
However, we also found that the interaction structure substantially affects how rapidly groups develop partner-specific conventions. 
Small groups may be able to successfully form conventions under limited feedback, but larger groups require thicker interaction structure.
Multi-player groups may therefore reveal key factors which are masked in purely dyadic settings.

<!-- ## Efficiency without semantic convergence  -->
Increasing efficiency, for example, has often been taken as an index of group-specific convention formation [@clark1986; @brennan1996; @yoon2014; @yoon2018]. 
In our work, however, we observe distinct patterns for measures of raw utterance length compared to the dynamics of semantic content. 
In Experiment 3, thin 6-person games showed much less group-specific divergence despite comparable accuracy and efficiency.
This gap raises the possibility that it is possible to become more efficient and accurate without negotiating a unified group-specific label. 
Instead, they may be relying more strongly on the group's priors [@guilbeault2021]. 
Thus, we encourage measures of semantic content (and not just performance) when evaluating convention formation. The transcripts for these games provide a rich dataset for exploring different ways language is used to form referential conventions. 

The causal mechanisms driving group size effects remain unclear. 
There are many differences between a 2-person group and a 6-person group that could plausibly lead to different outcomes. 
For example, in a dyad, producers can tailor their utterances to the one matcher, but in large groups, producers must balance the competing needs of different comprehenders [@schober1989; @yoon2018; @tolins2016]. These effects likely vary with the knowledge state and the communication channels available to comprehenders [@fox-tree2013; @horton2002; @horton2005]. 
Further work digging into the language used and the interactions between participants might unearth plausible mechanisms for how differences in group size and interaction structure influence outcomes, pointing towards future experimental conditions. 

<!-- ## Limitations and future directions.  -->
Even within the boundaries of the repeated reference game paradigm, there is a high-dimensional space of possible experiments. 
We sampled only a few points along a few salient dimensions. 
In our experiment 3, we grouped some factors together in order to run more games in each condition: a fully factorial design would have been too expensive to power adequately. We instantiated a "thin" channel by limiting matchers to 4 discrete utterances (emojis), but there are other possible restrictions that could be placed on the channel, such as rate-limited typing or explicit time pressure. 
Future work could explore other dimensions of the interaction structure, introducing pre-existing relationships or familiarity among group members, alternative incentives involving competition and power, or alternative referential targets involving more complex concepts.

<!-- ## Adding oral v etc -->
A particularly important dimension shaping interaction is the modality of communication, including whether whether the participants use oral or written language, whether they are co-present in the same space, and whether they have visual access to each others' faces and gestures.
Distinct modalities carry distinct affordances and norms. 
In this work, we relied on a text-based chat modality without allowing co-presence or visual access.  

We suspect that the general pattern of effects we see, in terms of group size and coherence, are likely to extend to other modalities. However, different modalities may allow for different strategies that may be more or less sensitive to group size, describer rotation, or different levels of matcher contributions. For instance, in face-to-face oral settings, it may be easier for describers to continuously talk until interrupted, or to monitor the comprehension of individual group members from their facial expressions. 
<!-- People communicate in a variety of ways in everyday life, and studying different of these situations is useful both for understanding the specific modalities and for understanding their shared communicative underpinnings. -->

<!-- ## Conclusion  -->
In conclusion, narrowly focusing on the settings that are easy to study in the lab -- dyads with rich communication channels -- can lead to theories that mispredict how interactions play out in multi-party groups. 
By studying common ground and coordination across a wider range of interaction structures, we can develop a more nuanced understanding of the obstacles that stand in the way of successful communication and how groups can overcome them.
This understanding can inform the design of policies and collaborative platforms that promote effective communication in various contexts, from small-scale conversations to large-scale civic discourse. 
As remote work and online communication become increasingly prevalent, it is increasingly crucial to understand how the structure of group communication environments shapes the effectiveness of human communication. 

# Materials and Methods

Our iterated reference task was implemented with Empirica [@almaatouq2020empirica], a React-based web development framework for real-time multi-player tasks. 
Our experiments were designed sequentially and pre-registered individually.^[Experiment 1: https://osf.io/cn9f4 for the 2-4 player groups, and https://osf.io/rpz67 for the 5-6 player data run later. Experiment 2: same describer at  https://osf.io/f9xyd, full feedback at  https://osf.io/j5zbm, and thin at  https://osf.io/k5f4t. Experiment 3: https://osf.io/untzy] 
We followed the pre-registered analysis plan for each experiment, although accuracy models were not explicitly specified until Experiment 3, and linguistic analyses were only verbally described starting with Experiment 2b. 
Results from some pre-registered models are omitted from the main text for brevity but are shown in the SI.
Exploratory mega-analytic models pooling across the three experiments were not pre-registered.

All materials, data, and analysis code is available at \url{https://github.com/vboyce/multiparty-tangrams}.

### Participants
This research was covered by the Stanford IRB under protocol 20009 "Online investigations of language learning". 
Participants were recruited using the Prolific platform. 
All participants self-reported as fluent native English speakers on Prolific's demographic prescreen. 
Experiment 1 took place between May and July 2021, Experiment 2 between March and August 2022, and Experiment 3 in October 2022. 
Each participant took part in only one experiment and was blocked from participating in subsequent experiments. 
As games with more participants tended to run longer, we paid participants different rates based on group size, with the goal of a consistent \$10 hourly rate.  Participants were paid \$7 for 2-player games, \$8.50 for 3-player games, \$10 for 4-player games, and \$11 for 5- and 6-player games. When one player occupied the describer role for the entirety of a 6-player game, they were rewarded an additional \$2 bonus. Across all games, participants could earn up to \$2.88 in performance bonuses.

A total of 1319 people participated across the 3 experiments. We recruited enough participants for 20 games in each condition in experiments 1 and 2 and 40 games per condition in experiment 3. However, due to attrition in filling the games initially and due to participants dropping out of the games, we ended up with fewer games in some conditions. For logistical reasons of matching participants into real-time games, we had to recruit participants in fairly large batches, and so did not have precise control to add new games to replace games that did not fill or had participants drop out early.  A breakdown of number of games and participants in each condition is shown in SI Table 3 along with further discussion of recruitment logistics. 

### Materials

The same 12 tangram images, drawn from @hawkins2020 and @clark1986, were used every block. 
These images were displayed in a 4 $\times$ 3 grid with the order randomized across participants to disincentivize spatial descriptions such as "top left," as the image might be in a different place on the describer's and matchers' screens.
To reduce cognitive load from visual search, the locations were fixed for each participant across trials.

### Procedure

The experimental procedure was very similar across the three experiments.
We first describe the procedure used in Experiment 1 and then describe the differences in later experiments. 

\paragraph{Experiment 1}
Participants were directed from Prolific to our custom web application, where they were presented with a consent form and a series of instruction pages explaining the protocol. 
After finishing the instructions, they needed to pass a quiz to proceed. 
They were then directed to a "waiting room" lobby.
Once the lobby filled to the required number of players, the game began. One lobby was filled before another was started; if a participant was waiting for 5 minutes, that lobby timed out, and the participant was paid without completing the experiment. Due to technical constraints with assigning participants to lobbies and games, only games of a single experimental condition could be active at a time. Thus, different conditions were run on different days or times of day.   
One of the participants was randomly selected to begin in the role of describer, and the other participants were assigned to the role of matchers. 
On each trial, the describer saw a fixed array of tangrams with one tangram (privately) highlighted as the *target*. 
They were given a chat interface to communicate the target to the matchers, who were asked to determine which of the 12 images was the referential target.
All participants were free to use the chat box to communicate at any time, but matchers could only make a selection after the describer had sent a message. 
Once a matcher clicked, they could not change their selection. 
There was no signal to the describer or other matchers about who had already made a selection. 
We recorded what all participants said in the chat, as well as who selected which image and how long they took to make their selections. 

Once all matchers had made a selection (or a 3-minute timer ran out), participants were given feedback and proceeded to the next trial. 
Matchers only received *binary* feedback about whether they had chosen correctly or not; that is, matchers who made an incorrect choice were not shown the correct answer (see SI Figure 1 for example feedback). 
The describer saw which tangram each matcher selected, but matchers did not see one another's selections. 
Matchers got 4 points for each correct answer; the describer got points equal to the average of the matchers' points.
These points were translated into performance bonuses at the end of the experiment (1 point = 1 cent bonus). 
After the describer had described each of the 12 images as targets, in a randomized sequence, the process repeated with the same set of targets, for a total of 6 such repetition blocks (72 trials). 

The same person was the describer for an entire block, but participants rotated roles between blocks. 
Thus, over the course of the 6 blocks, participants were describers 3 times in 2-player games, twice in 3-player games, once or twice in 4 and 5-player games, and once in 6-player games. 
Rotating the describer was chosen in this first experiment to keep participants more equally engaged (the describer role is more work), and to provide a more robust test of our hypotheses regarding efficiency and convention formation. 
After the game finished, participants were given a survey asking for optional demographic information and feedback on their experience with the game. 

If a participant disconnected from the experiment, the game would stop. 

\paragraph{Experiment 2}
Experiment 2 consisted of three different variations on Experiment 1, all conducted in 6-player games. 
Each of these conditions differed from the Experiment 1 baseline in exactly one way. 
In the *same describer* condition, one person was designated the describer for the entire game, rather than having the describer role rotate. 
In the *full feedback* condition, all participants were shown what all others had selected as well as the identity of the correct target.
This condition was similar to previous dyadic work, such as @hawkins2020, where the correct answer was indicated during feedback. 
In the *thin* condition, we altered the chatbox interface for matchers. 
Instead of a textbox, matchers had 4 buttons, each of which sent a different emoji to the chat. Matchers were given suggested meanings for the 4 emojis during the instruction phase. They could send as many emojis as desired; for instance, they might initially indicate confusion, and later indicate understanding. In addition, for the thin condition, we added notifications that appeared in the chat box marking the time when each player had made a selection. 

\paragraph{Experiment 3}
The thin channel condition in Experiment 3 was the same as the thin condition in Experiment 2. 
The thick condition combined the two coherency-enhancing variations from Experiment 2: the same participant remained in the describer role throughout, and full feedback was given about the correct answer and what all other players had selected. 
Across both conditions in Experiment 3, notifications were sent to the chat to indicate when a participant had made a selection. 
For experiment 3, game lobbies worked slightly differently, and 5 minutes after the first participant had joined the lobby, the game started if there were at least two participants. Correspondingly, in experiment 3, games did not stop if a player disconnected, instead if there were at least two players still active, the game continued, swapping a player into the role of describer if necessary to continue the game.  

### Data pre-processing and exclusions

Participants could use the chat box freely, which meant that the chat transcript contained some non-referential language. The first author skimmed the chat transcripts, tagging utterances that did not refer to the current tangram. These were primarily pleasantries ("Hello"), meta-commentary about how well the task was going, and bare confirmations or denials ("ok", "got it", "yes", "no"). We excluded these utterances from our analyses. Note that chat lines sometimes included non-referential words in addition to words referring to the tangrams ("ok, so it looks like a zombie", "yes, the one with legs"); these lines were retained intact. 

In Experiments 1 and 2, games did not start if there were not enough participants and ended if any participant disconnected. 
In Experiment 3, games started after a waiting period even if they were not entirely full and continued even in the event that a participant disconnected (with describer role reassigned if necessary), unless the game dropped below 2 players. 
The distribution of player counts in games that were initially recruited to be 6 player games is shown in SI Figure 3. 
The realities of online recruitment and disconnection meant that the number of games varied between conditions. 
We excluded incomplete blocks from analyses, but included complete blocks from partial games (See SI Table 3).
When skimming transcripts to tag non-referential utterances, we noticed that one game in the 6-player thick condition had a describer who did not give any sort of coherent descriptions, even with substantial matcher prompting. We excluded this game from analyses. 

### Modelling strategy

We fit all regression models in brms [@burkner2018] with weakly regularizing priors.
We were unable to fit the full pre-registered mixed effects structure in a reasonable amount of time for some models, so we included the maximal  hierarchical effects that were tractable. 
All model results and priors and formulae are reported in the SI.
Models of accuracy used by-group random intercepts only, models of word count used full mixed effect structure, and models of S-BERT similarities used by-group and by-target random intercepts as applicable (see SI Figure 11).
Models of matcher accuracy were logistic models with normal(0,1) priors for betas and sd.
Models of describer efficiency were run as linear models with an intercept prior of normal(12,20), a beta prior of normal(0,10), an sd prior of normal(0,5) and a random-effect correlation prior of lkj(1). 
For all of the models of SBERT similarity, we used linear models with the priors normal(.5,.2) for the intercept, normal(0,.1) for betas, and normal(0,.05) for sd. 
As an additional post-hoc analysis, we ran mega-analytic models combining data across all experiments. 
For these models, we grouped the 3 thin-ish conditions (2c, and the two thin conditions of experiment 3) as one level, and coded the rest of the conditions as thick-ish. 
Game size was coded as a continuous measure (2 through 6). The priors for the mega-analytic models were the same as for the per-experiment models described above. 

As a sensitivity analysis, we re-ran the primary models on the subset of the data from games that a) completed all 72 trials and b) had the full complement of players the entire time (relevant to 6-player experiment 3 games where games could start or continue with fewer players). Discrepancies are mentioned in the results, and these analyses are depicted in SI Figures 4 and 5 and SI Tables 54-73. 
We also needed to decide how to handle dropout in Experiment 3, as some of the 6-player games did not retain all 6 players for the entire game. 
Our decision was to follow an intent-to-treat analysis and treat data as missing completely at random. 
Note that this choice underestimates differences between 2-player and (genuine) 6-player games by labeling some smaller groups as 6-player groups.
We do not know exactly what leads some participants to drop out, but it is possible that some factors may be random (ex. connection issues) and others may be correlated with performance (ex. frustration because group is struggling).   
We do not know whether groups that start and continue at the full size differ from games where some participants drop out. 
This is potentially an issue across all experiments; in experiments 1 and 2, groups stopped playing if anyone dropped out, and in experiment 3 they kept playing as a smaller group. 
The number of games in each condition and rates of dropoff are shown in SI Table 3 and SI Figure 3. 

# Author's Note
Experiment 1 was previously reported in Proceedings of the Annual Meeting of the Cognitive Science Society 44 (2022). 

We thank the LangCog Lab, Saxelab, CAMP 5, HSP 2023, and Cogsci 2022 audiences for helpful feedback on this work. 

This work was supported by a Hoffman-Yee Grant from the Stanford Institute for Human-Centered AI.

We report CRediT taxonomy contributions as follows: All authors did conceptualization and methodology. VB did data curation, formal analysis, investigation, visualization, and writing -- original draft. VB and RDH did software. RDH, NDG, and MCF did writing -- editing. MCF and NDG did funding. MCF did supervision. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent


